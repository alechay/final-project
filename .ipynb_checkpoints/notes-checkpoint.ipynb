{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Overview of classification methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the method implement discriminative learning of linear classifiers under convex loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Nearest Neighbors classifier**\n",
    "\n",
    "http://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification\n",
    "\n",
    "\n",
    "\n",
    "* **Logistic Regression**\n",
    "\n",
    "http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "\n",
    "\n",
    "\n",
    "* **Neural networks**:\n",
    "    - Simplest example - Perceptron is very similar to logistic regression  http://scikit-learn.org/stable/modules/linear_model.html#perceptron\n",
    "   \n",
    "\n",
    "\n",
    "* **Linear and Quadratic Discriminant Analysis**\n",
    "http://scikit-learn.org/stable/modules/lda_qda.html\n",
    "\n",
    "    - Advantage: closed-form solution, no hyperparameters, can extend to multiclass classification.\n",
    "\n",
    "    - Disadvantage: assumptions about distibutions: Gaussian\n",
    "\n",
    "\n",
    "* **Support Vector Machines** - SVM Classifier: SVC http://scikit-learn.org/stable/modules/svm.html#classification\n",
    "\n",
    "    Optimize a hinge loss functions that defines the width of the decision boundary separating the classes. Support vectors are the examples from two classes closest to the boundary. The larger the boundary - the better. \n",
    "    Advantages include linear and nonlinear options with different kernels. Disadvantage is compuational complexity, does not scale well.\n",
    "    \n",
    "\n",
    "* A generalized version of linear classifiers **Stochastic Gradient Descent**:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "SGDC has a choice of loss functions: hinge (as in SVM), log (as in logistic regression), ... and supports regularization.\n",
    "SGDC updates by calculating the gradient for each example separately, so it can be trained on very large datasets: out-of-core training with .partial_fit().\n",
    "\n",
    "\n",
    "* **Decision tree**\n",
    "http://scikit-learn.org/stable/modules/tree.html#classification\n",
    "\n",
    "\n",
    "* **Ensemble methods**:\n",
    "\n",
    "    - **Gradient Boosting** approach  http://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting\n",
    "\n",
    "    - Bagging approaches:\n",
    "\n",
    "        - **Voting**\n",
    "        http://scikit-learn.org/stable/modules/ensemble.html#voting-classifier\n",
    "\n",
    "        - **RandomForest**\n",
    "        http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "\n",
    "* **Naive Bayes** classifier:\n",
    "http://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes\n",
    "\n",
    "    NB Can be extended to multiclass probelems. Can handle out-of-core training on large datasets easily.\n",
    "\n",
    "\n",
    "* For time series there exist different methods, like **Gaussian process** classifier:\n",
    "http://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process-classification-gpc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision (Positive predictive value)\n",
    "Out of all predicted positive, how many are actually positive?\n",
    "$$Precision = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "#### True positive rate, also Sensitivity or Recall\n",
    "Out of all actual positive cases, how many do we predict as positive?\n",
    "$$ TPR = Sensitivity = Recall = \\frac{TP}{TP + FN} $$\n",
    "\n",
    "#### F score ($F_1$ score, F-measure)\n",
    "\n",
    "F score is a combination of precision and recall:\n",
    "\n",
    "$$F_1 = \\frac{2TP}{2TP + FP + FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although here we have a balanced dataset with approx. equal support for each class, note the micro and macro averaging techniques:\n",
    "\n",
    "**\"macro\"** simply calculates the mean of the binary metrics, giving equal weight to each class. In problems where infrequent classes are nonetheless important, macro-averaging may be a means of highlighting their performance. On the other hand, the assumption that all classes are equally important is often untrue, such that macro-averaging will over-emphasize the typically low performance on an infrequent class.\n",
    "\n",
    "\n",
    "**\"weighted\"** accounts for class imbalance by computing the average of binary metrics in which each classâ€™s score is weighted by its presence in the true data sample.\n",
    "\n",
    "\n",
    "**\"micro\"** gives each sample-class pair an equal contribution to the overall metric (except as a result of sample-weight). Rather than summing the metric per class, this sums the dividends and divisors that make up the per-class metrics to calculate an overall quotient. Micro-averaging may be preferred in multilabel settings, including multiclass classification where a majority class is to be ignored."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
